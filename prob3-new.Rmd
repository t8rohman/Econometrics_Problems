---
title: "Problem 3 - Sales and Inventory"
author: "Taufiqur Rohman"
date: '2022-05-26'
output:
  pdf_document: default
  word_document: default
  html_document: default
---

This is the last question that was given from my professor for my econometrics homework. The problem is to compare the time series linear regression, accounting auto collinearity in the model, and how to eliminate it by transforming the data. Before I start, I will upload first the "SalesInventory.xls" data that will be used for this problem. Then, I will explore a little bit about the data by plotting a graph.

```{r library, include=FALSE}
library(ggplot2)
library(readxl)
library(broom)
library(ggpubr)
library(lmtest)
```

```{r upload data set}
df_prob3 <- read_excel("~/Documents/RU/Econometrics II/HW-05-May/SalesInventory.xls")
```

```{r define x and y variable}
y <- ts(df_prob3$Sales)
x <- ts(df_prob3$Inventories)
```

```{r explore data by graph}
graph1 <- ggplot(df_prob3, aes(x = Year, y = Inventories)) +
  geom_line() + 
  xlab("Year")

graph2 <- ggplot(df_prob3, aes(x = Year, y = Sales)) +
  geom_line() + 
  xlab("Year")

graph1
graph2
```


# Problem 1

Estimate the preceding regression!

```{r linear regression}
p1 <- lm(Inventories ~ Sales, data = df_prob3)
summary(p1)
```


# Problem 2

From the estimated residuals find out if there is positive autocorrelation using the Durbinâ€“Watson test and AR(1) test.

```{r durbin-watson test}
p2a <- dwtest(formula = p1,  alternative = "two.sided")
p2a
```


# Problem 3

If you suspect that the auto regressive error structure is of order p, how would you choose the order of p?

```{r using pacf test for inventory and sales}
pacf(residuals(p1))
```

Using partial auto correlation function (PACF) in the residual of the linear model, i found that there is an auto regressive error structure in order of p = 1.  


# Problem 4

On the basis of the results of this test, how would you transform the data to remove autocorrelation? Take the appropriate corrective action, present the revised results, and compare this with the previous one.

```{r autoregressive in residual model}
ar1res <- arima(residuals(p1), order = c(1,0,0))
summary(ar1res)
```

Using AR(1) model to the residual, I found the intercept = 0.9427. Then, I input the value to create the new model to eliminate the autocorrelation. I calculate variables to use in the adjustment regression.

```{r transform the data}
xl <- ts.intersect(x, lag(x,-1)) # Create matrix with x and lag 1 x as elements
xnew <- xl[,1] - 0.9427*xl[,2] # Create x variable for adjustment regression
yl <- ts.intersect(y,lag(y,-1)) # Create matrix with y and lag 1 y as elements
ynew <- yl[,1] - 0.9427*yl[,2] # Create y variable for adjustment regression
 
adjustreg <- lm(ynew ~ xnew) # Adjustment regression
summary(adjustreg)
```

The slope estimate (5,680) and its standard error (2,263) are the adjusted estimates for the original model.

```{r evaluate pacf model}
pacf(residuals(adjustreg))
```

Evaluating the new model using the PACF, I successfully eliminated the autocorrelation in the data.

# Problem 5

Use log form, and try (1)-(4).

```{r create new df with logged variables}
df_prob3['logsales'] <- log(df_prob3['Sales'])
df_prob3['loginvent'] <- log(df_prob3['Inventories'])

y2 <- ts(df_prob3['loginvent'])
x2 <- ts(df_prob3['logsales'])
```

```{r explore log data by graph}
graphlog_1 <- ggplot(df_prob3, aes(x = Year, y = logsales)) +
  geom_line() + 
  xlab("Year")

graphlog_2 <- ggplot(df_prob3, aes(x = Year, y = loginvent)) +
  geom_line() + 
  xlab("Year")

graphlog_1
graphlog_2
```

```{r linear regression - logged}
p5a <- lm(loginvent ~ logsales, data = df_prob3)
summary(p5a)
```

```{r durbin-watson test - logged}
p5b <- dwtest(formula = p5a,  alternative = "two.sided")
p5b
```

```{r using pacf test to check autocorrelation - logged}
pacf(residuals(p1))
```

Using partial auto correlation function (PACF) in the residual of the linear model, I found that there is an auto regressive error structure in order of p = 1.  

```{r autoregressive in residual model - logged}
ar1res2 <- arima(residuals(p5a), order = c(1,0,0))
summary(ar1res2)
```

Using AR(1) model to the residual for the logged data, I found the intercept = 0.8893. Then, I input the value to create the new model to eliminate the autocorrelation. I calculate variables to use in the adjustment regression.

```{r transform the data - logged}
xl2 <- ts.intersect(x2, lag(x2,-1)) # Create matrix with x and lag 1 x as elements
xnew2 <- xl2[,1] - 0.9427*xl2[,2] # Create x variable for adjustment regression
yl2 <- ts.intersect(y2,lag(y2,-1)) # Create matrix with y and lag 1 y as elements
ynew2 <- yl2[,1] - 0.9427*yl2[,2] # Create y variable for adjustment regression
 
adjustreg2 <- lm(ynew2 ~ xnew2) # Adjustment regression
summary(adjustreg2)
```

The slope estimate (0.161) and its standard error (0.095) are the adjusted estimates for the original model.

```{r evaluate pacf model - logged}
pacf(residuals(adjustreg2))
```

Evaluating the new model using the PACF in logged data, I successfully eliminated the autocorrelation.